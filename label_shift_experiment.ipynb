{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "label_shift_experiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2DTAIcXaqiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "## A function which takes the img_size, prob_vector and the number of desired samples and pytorch tensor and labels\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "def generate_img_data(prob_vector, num_samples, img_size = (50, 50)):\n",
        "\n",
        "  \"\"\"\n",
        "    Generate images with circles inside as data\n",
        "    prob_vector: The probability for the different classes\n",
        "                  (note that this is hard-coded to three classes: big circle, medium circle and small circle)\n",
        "    num_samples: Number of desired samples\n",
        "    img_size: Size of img\n",
        "  \"\"\"\n",
        "\n",
        "  #num_samples_1_train = 5000 \n",
        "  store_dataset_x = np.zeros((num_samples, img_size[0], img_size[1]))\n",
        "  store_dataset_y = np.zeros(num_samples)\n",
        "\n",
        "  num_classes = len(prob_vector)\n",
        "  #circle_prob_train = np.array([0.2, 0.4, 0.4])\n",
        "  ind_vec_train = [np.random.choice([kkk for kkk in range(num_classes)], p = prob_vector) for iii in range(num_samples)]\n",
        "\n",
        "  for kkk in range(num_samples):\n",
        "\n",
        "    ## Generate random center position of the circle\n",
        "    centre_row = np.random.randint(15,35)\n",
        "    centre_col = np.random.randint(15,35)\n",
        "\n",
        "    ## Generate circle with big, medium or small radius\n",
        "    if ind_vec_train[kkk] == 0:\n",
        "      img_rad = np.random.randint(2,4)\n",
        "    if ind_vec_train[kkk] == 1:\n",
        "      img_rad = np.random.randint(8,12)\n",
        "    if ind_vec_train[kkk] == 2:\n",
        "      img_rad = np.random.randint(14,18)\n",
        "    \n",
        "    img = np.zeros(img_size)\n",
        "    img = cv2.circle(img, (centre_row, centre_col), img_rad, 1, 2)\n",
        "    store_dataset_x[kkk,:,:] = img\n",
        "    store_dataset_y[kkk] = ind_vec_train[kkk]\n",
        "\n",
        "  return store_dataset_x, store_dataset_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7L5n3FgfAeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Just a function to plot some samples to see what it looks like\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_img(img_array, num_plots = 4):\n",
        "    fig, ax = plt.subplots(1, num_plots, figsize=(16,8))\n",
        "    plot_ind = np.random.randint(0, img_array.shape[0], 4)\n",
        "    for jjj in range(num_plots):\n",
        "      ax[jjj].imshow(img_array[plot_ind[jjj],:,:])\n",
        "\n",
        "    fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6xg9KL5gN3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def create_data_loader(img_array, img_labels, batch_size = 64):\n",
        "  x_torch = torch.from_numpy(img_array).float()\n",
        "  x_torch = torch.unsqueeze(x_torch, 1)\n",
        "  y_torch = torch.from_numpy(img_labels).long()\n",
        "  data_list = [(x_torch[kkk,:,:,:], y_torch[kkk]) for kkk in range(x_torch.shape[0])]\n",
        "  data_loader = torch.utils.data.DataLoader(data_list, batch_size = batch_size, shuffle = True)\n",
        "  return data_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41uywzC_ggU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 100, 5)\n",
        "    self.conv2 = nn.Conv2d(100, 60, 5)\n",
        "    self.conv3 = nn.Conv2d(60, 20, 5)\n",
        "    self.fc1 = nn.Linear(500, 100)\n",
        "    self.fc2 = nn.Linear(100, 3)\n",
        "    self.T = nn.Parameter(torch.tensor(1.))\n",
        "    self.bias_t = nn.Parameter(torch.zeros(3))\n",
        "\n",
        "  def train_net(self):\n",
        "    for param in self.named_parameters():\n",
        "      \n",
        "      if param[0] in ['T', 'bias_t']:\n",
        "        param[1].requires_grad = False\n",
        "  \n",
        "  def calibrate(self):\n",
        "    \n",
        "    for param in self.named_parameters():\n",
        "      if param[0] in ['T', 'bias_t']:\n",
        "        param[1].requires_grad = True\n",
        "      else:\n",
        "        param[1].requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, (2, 2))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, (2, 2))\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = x.view(x.shape[0],-1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)/self.T + self.bias_t ## For calibration later\n",
        "    x = F.log_softmax(x, dim = 1)\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_T5BUWugy-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(my_net, train_loader, val_loader, max_epoch = 2, calibrate = False):\n",
        "\n",
        "  if not calibrate:\n",
        "    my_net.train_net()\n",
        "    opt = torch.optim.SGD(my_net.parameters(), lr = 1e-2, momentum = 0.9)\n",
        "    loss_fn = nn.NLLLoss()\n",
        "\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "\n",
        "      for data, anno in train_loader:\n",
        "        \n",
        "        my_net.zero_grad()\n",
        "        out = my_net(data)\n",
        "        loss = loss_fn(out, anno)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        acc_v = 0\n",
        "        for data, anno_v in val_loader:\n",
        "          my_net.zero_grad()\n",
        "          out_v = my_net(data)\n",
        "          loss_v = loss_fn(out_v, anno_v)\n",
        "          values, indices = torch.max(out_v, 1)\n",
        "          acc_v = acc_v + torch.sum(indices == anno_v)\n",
        "        acc_v = acc_v.float() / len(val_loader.dataset)\n",
        "\n",
        "\n",
        "      print('Epoch {} Train Loss {} Val Loss {} Val Acc {}'.format(epoch, loss.item(), loss_v.item(), acc_v))\n",
        "\n",
        "  if calibrate: \n",
        "\n",
        "    my_net.calibrate()\n",
        "\n",
        "    opt = torch.optim.SGD(my_net.parameters(), lr = 1e-5, momentum = 0.9)\n",
        "    loss_fn = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(10):\n",
        "      acc_v = 0\n",
        "      for data, anno_v in val_loader:\n",
        "        my_net.zero_grad()\n",
        "        out_v = my_net(data)\n",
        "        loss_v = loss_fn(out_v, anno_v)\n",
        "        loss_v.backward()\n",
        "        opt.step()\n",
        "        values, indices = torch.max(out_v, 1)\n",
        "        acc_v = acc_v + torch.sum(indices == anno_v)\n",
        "      acc_v = acc_v.float() / len(val_loader.dataset)\n",
        "      print('Val loss {} Val Acc {}'.format(loss_v, acc_v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQPpwOFJjkwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def estimate_class_probabilities(my_net, test_loader, simulated_prob):\n",
        "\n",
        "  store_probs = []\n",
        "  for idx, (data, anno_s) in enumerate(test_loader):\n",
        "\n",
        "    prob = my_net(data)\n",
        "    if idx == 0:\n",
        "      store_prob = torch.exp(prob)\n",
        "    else:\n",
        "      store_prob = torch.cat((store_prob, torch.exp(prob)))\n",
        "  ## To get all the probabilities as a numpy array\n",
        "  prob_source_target = store_prob.detach().numpy()\n",
        "\n",
        "  ## The algorithm is performed below:\n",
        "  target_prob = simulated_prob ## Init\n",
        "\n",
        "  num_iters = 50\n",
        "  eps = 1e-8\n",
        "\n",
        "  for kkk in range(num_iters):\n",
        "\n",
        "    store_target_prob = []\n",
        "\n",
        "    for aaa in range(prob_source_target.shape[0]):\n",
        "\n",
        "      p1 = np.log(target_prob) + np.log(prob_source_target[aaa,:] + eps)  - np.log(simulated_prob)\n",
        "      p2 = np.sum(np.exp(np.log(target_prob) + np.log(prob_source_target[aaa,:] + eps)  - np.log(simulated_prob)))\n",
        "      prob_targ_cond = np.exp(p1 - np.log(p2))\n",
        "      store_target_prob.append(prob_targ_cond)\n",
        "      \n",
        "    all_probs = np.array(store_target_prob)\n",
        "    target_prob = np.mean(all_probs, 0)\n",
        "\n",
        "  return target_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_RzLKHnVDK",
        "colab_type": "text"
      },
      "source": [
        "The setup is as follows: \n",
        "* We have a simulated dataset $(\\tilde{x}_i,a_i)$ with consisting of images and their corresponding annotations. These samples are assumed to follow a joint distribution $p(\\tilde{x},a)$ which induces a marginal distribution over the annotations $p(a)$\n",
        "* We have an observed dataset with images $(x_i)$ with unobserved annotations sampled from a joint distribution $Q(x,a)$ which induces a marginal distribution $q(a)$.\n",
        "* The goal is to determine the difference between $p(a)$ and $q(a)$.\n",
        "\n",
        "We can do this with an assumption of label shift. The main assumption here is that $p(x|a) = q(x|a)$ (with QD and SD thise assumption would be that given an annotation the distribution for the images would be the same which to me does not hold since the images look different, but we might be able to argue instead that $p(f(x)|a) = q(f(x)|a)$ where $f$ is the neural network mapping into a latent space. So given an annotation, the distribution in the latent space should be the same for the two datasets which might be sort of true if we can assume that the network extracts some underlying characteristics of our data) and that the difference in the joint distribution arises from the fact that $p(a) \\neq q(a)$.\n",
        "\n",
        "Lets try a simple example and see how it works. \n",
        "* We create an image dataset where we have circles with three possible classes: Big circle, medium circle and small circle.\n",
        "* We create a set to act as SD where we have the probabilities [0.2, 0.4,0.4] for the three classes and a set to act as QD (note that $G=G^*$ here) with probabilities [0.2,0.7,0.1].\n",
        "* Goal is to obtain an estimate of the distribution [0.2,0.7,0.1] over the QD set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY75ktOvcwy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "outputId": "181f2acf-1385-485d-dfc1-a1ed86946f1d"
      },
      "source": [
        "## This is the main file then\n",
        "\n",
        "## The probabilities for the three classes\n",
        "##    Small circle, medium circle, big circle\n",
        "simulate_prob_vec = [0.2,0.4,0.4] ## This is the probabilities for the simulated data\n",
        "observed_prob_vec = [0.2,0.7,0.1] ## These are the probabilities we want to infer\n",
        "\n",
        "\n",
        "num_train_samples = 5000\n",
        "num_val_samples = 1000\n",
        "num_test_samples = 15000\n",
        "\n",
        "x_train, y_train = generate_img_data(simulate_prob_vec, num_train_samples)\n",
        "x_val, y_val = generate_img_data(simulate_prob_vec, num_val_samples)\n",
        "x_test, y_test = generate_img_data(observed_prob_vec, num_val_samples)\n",
        "\n",
        "## Just to see the dataset\n",
        "## There is no difference between G and G* now.\n",
        "plot_img(x_train) \n",
        "plot_img(x_val)\n",
        "plot_img(x_test)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAADjCAYAAADt28tlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGk8AAAC60ngCAADQ1f8BogSLlHwF54MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1152x576 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAADjCAYAAADt28tlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1152x576 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAADjCAYAAADt28tlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1152x576 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBqAwBoddhGY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "4b1d77f0-4759-489d-951c-1d11b3e43543"
      },
      "source": [
        "## Train the network and calibrate it\n",
        "train_loader = create_data_loader(x_train, y_train)\n",
        "val_loader = create_data_loader(x_val, y_val)\n",
        "test_loader = create_data_loader(x_test, y_test)\n",
        "\n",
        "my_net = Network()\n",
        "train_network(my_net, train_loader, val_loader, calibrate = False)\n",
        "print('Calibrating the network')\n",
        "train_network(my_net, train_loader, val_loader, calibrate = True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Train Loss 0.11778327077627182 Val Loss 0.04885342717170715 Val Acc 0.9860000014305115\n",
            "Epoch 1 Train Loss 0.00048001279355958104 Val Loss 0.0022883680649101734 Val Acc 1.0\n",
            "Calibrating the network\n",
            "Val loss 0.00040808686753734946 Val Acc 1.0\n",
            "Val loss 0.00028041511541232467 Val Acc 1.0\n",
            "Val loss 0.0005068840691819787 Val Acc 1.0\n",
            "Val loss 0.0010371885728091002 Val Acc 1.0\n",
            "Val loss 0.0006770615582354367 Val Acc 1.0\n",
            "Val loss 0.0013925096718594432 Val Acc 1.0\n",
            "Val loss 0.0005729129770770669 Val Acc 1.0\n",
            "Val loss 0.0005144446040503681 Val Acc 1.0\n",
            "Val loss 0.0014219331787899137 Val Acc 1.0\n",
            "Val loss 0.008515631780028343 Val Acc 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWS_tP49gdDv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8b57603a-c2a8-4eb2-8416-239a32c14741"
      },
      "source": [
        "## Estimate the annotation probabilties\n",
        "estimated_prob = estimate_class_probabilities(my_net, test_loader, simulate_prob_vec)\n",
        "print('The true probabilities are {}'.format(observed_prob_vec))\n",
        "print('The estimated probabilities are {}'.format(estimated_prob))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The true probabilities are [0.2, 0.7, 0.1]\n",
            "The estimated probabilities are [0.21178482 0.6925973  0.09561789]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu11hMkvkhRJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71cdf8b6-1707-4cd5-b31f-fcaaacd115a7"
      },
      "source": [
        "## We can now gauge the differences in the annotation distribution between the two sets\n",
        "prob_diff = np.abs(estimated_prob - simulate_prob_vec)\n",
        "print(prob_diff) ## Values close to 0 indicate our expectations coincide with the reality for that annotation."
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01178482 0.2925973  0.30438211]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYRkEJlqlWx5",
        "colab_type": "text"
      },
      "source": [
        "So in this simple example we could see that the method gives quite good results. Some issues and questions:\n",
        "* Now I only worked with three annotations, big, medium, small. For this setting it is easy to construct a network that predicts these three annotations well from the images. But if we have many annotations, like brightness can range from 0-255, bounding box corner can range from 0-20 etc. This creates a huge classification problem for the network. And I think we somehow need it to be a classification problem because the method relies on $p(a|x)$ being probabilities. So if one wants to train a network to predict bounding box coordinates with mean squared loss then one has to find a way to convert the corresponding values to probabilities before applying the algorithm.\n",
        "* A possible solution to the above problem would be to work with annotations on different levels. So create \"large\" annotation classes and see if the expectations on the simulated dataset match those on the observed dataset first. And then find the areas where the expectations are mismatched and explore those further. For example, in the example above we could see that our expectation for the small circle class have been met and thus we can maybe just continue with investigating the large and medium class and create/modify a network to predict these better. This could be done by creating new classes such as \"medium circles in top right corner, medium circle in bottom left, etc\" and then apply the same method and see where the expectations differ now.\n",
        "* Now there is no difference in $G$ and $G^*$ for the train and test set but this might affect the end results as well.\n",
        "* I also mentioned in the beginning that we might be able to assume $p(f(x)|a) = q(f(x)|a)$ but maybe one needs to think more about this.\n",
        "* Also the dataset here might be a bit too easy and that is why the results are so good here.\n"
      ]
    }
  ]
}